import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns
import pennylane as qml
from pennylane import numpy as pnp
from sklearn.decomposition import PCA

# Load the labels_train.csv data
labels_train_path = 'labels_train.csv'  # Adjust this path as per your file location
labels_train = pd.read_csv(labels_train_path)

# Display the first few rows of labels_train
print(labels_train.head())

# Extract features and target variable
features = labels_train[['xmin', 'xmax', 'ymin', 'ymax']].values
target = labels_train['class_id'].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Check the shapes of the datasets
print("Shapes - X_train:", X_train.shape, "y_train:", y_train.shape)
print("Shapes - X_test:", X_test.shape, "y_test:", y_test.shape)

# Train a Support Vector Classifier
svc = SVC(kernel='linear', random_state=42)
svc.fit(X_train, y_train)

# Predict on the test set
y_pred = svc.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print classification report
print(f'Accuracy: {accuracy:.2f}\n')
print('Classification Report:\n', class_report)

# Plot the confusion matrix
plt.figure(figsize=(10, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title(f'Confusion Matrix\nAccuracy: {accuracy:.2f}')
plt.show()

# Define a quantum device with PennyLane
num_qubits = 4
dev = qml.device('default.qubit', wires=num_qubits)

# Define the quantum circuit
def circuit(params, x):
    qml.AngleEmbedding(x, wires=range(num_qubits))
    qml.BasicEntanglerLayers(params, wires=range(num_qubits))
    return [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]  # Output values for the classes

# Create a QNode
@qml.qnode(dev)
def quantum_classifier(params, x):
    return circuit(params, x)

# Define the variational classifier
def variational_classifier(params, x):
    return quantum_classifier(params, x)

# Initialize the parameters
np.random.seed(42)
params = np.random.randn(6, num_qubits)  # Adjusted for the number of layers and qubits

# Define the cost function
def cost(params, X, Y):
    predictions = pnp.array([variational_classifier(params, x) for x in X])
    return pnp.mean((predictions - Y) ** 2)

# Convert labels to one-hot encoding
y_train_one_hot = np.zeros((y_train.size, y_train.max() + 1))
y_train_one_hot[np.arange(y_train.size), y_train] = 1

# Training using gradient descent
opt = qml.AdamOptimizer(stepsize=0.01)
num_epochs = 100

for epoch in range(num_epochs):
    params, cost_value = opt.step_and_cost(lambda p: cost(p, X_train, y_train_one_hot), params)
    predictions = [variational_classifier(params, x) for x in X_train]
    acc = np.mean(np.argmax(predictions, axis=1) == y_train)
    print(f"Epoch: {epoch+1} | Cost: {cost_value:.4f} | Accuracy: {acc:.4f}")

# Predict on the test set
y_pred_quantum = [np.argmax(variational_classifier(params, x)) for x in X_test]

# Evaluate the VQC model
accuracy = accuracy_score(y_test, y_pred_quantum)
class_report = classification_report(y_test, y_pred_quantum)
conf_matrix = confusion_matrix(y_test, y_pred_quantum)

# Print classification report
print(f'Accuracy: {accuracy:.2f}\n')
print('Classification Report:\n', class_report)

# Plot the confusion matrix
plt.figure(figsize=(10, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title(f'Confusion Matrix\nAccuracy: {accuracy:.2f}')
plt.show()

# Reduce the dataset to 2 dimensions using PCA for visualization purposes
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(features)

# Split the reduced data into training and testing sets
X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, target, test_size=0.2, random_state=42)

# Define the quantum classifier for reduced dimensions
def variational_classifier_2d(params, x):
    qml.AngleEmbedding(x, wires=range(2))
    qml.BasicEntanglerLayers(params, wires=range(2))
    return [qml.expval(qml.PauliZ(i)) for i in range(2)]  # Output for reduced dimensions

@qml.qnode(dev)
def quantum_classifier_2d(params, x):
    return variational_classifier_2d(params, x)

# Initialize parameters for 2D
params_2d = np.random.randn(6, 2)

# Training for 2D
for epoch in range(num_epochs):
    params_2d, cost_value = opt.step_and_cost(lambda p: cost(p, X_train_red, y_train_red), params_2d)
    predictions = [variational_classifier_2d(params_2d, x) for x in X_train_red]
    acc = np.mean(np.argmax(predictions, axis=1) == y_train_red)
    print(f"Epoch: {epoch+1} | Cost: {cost_value:.4f} | Accuracy: {acc:.4f}")

# Create a plot to visualize decision boundaries
def plot_decision_boundaries(X, y, model, params, ax, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))
    Z = [np.argmax(model(params, x)) for x in np.c_[xx.ravel(), yy.ravel()]]
    Z = np.array(Z).reshape(xx.shape)
    ax.contourf(xx, yy, Z, alpha=0.3)
    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', marker='o')
    ax.set_title(title)

fig, ax = plt.subplots(figsize=(10, 6))
plot_decision_boundaries(X_test_red, y_test_red, quantum_classifier_2d, params_2d, ax, 'Decision Boundaries (PCA-reduced data)')
plt.show()
